{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not find GPU\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "from dare import Riccati\n",
    "from data_gen import generate_simulated_data\n",
    "from device import device\n",
    "from env_setup import *\n",
    "from human import HumanRobotEnv\n",
    "from models import ThetaEstimatorTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 65.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate trajectories to train estimator\n",
    "sim_policy = dict()\n",
    "sim_policy['human_state'] = 'varying'  # can be 'fixed' or 'varying'\n",
    "sim_policy['mental_state'] = 'varying'  # can be 'fixed' or 'varying'\n",
    "\n",
    "# sim_policy['human_state_init'] = [[1.3], [0.3]]\n",
    "# sim_policy['human_state_init'] = [[0.4], [0.0]] # only needed when human_state = 'fixed'\n",
    "sim_policy['mental_state_init'] = [[1.0]] # only needed when mental_state = 'fixed'\n",
    "\n",
    "sim_time = 20\n",
    "n_demo = 100\n",
    "is_updating_internal_model = True\n",
    "stochastic_human = True\n",
    "influence_type = 'oracle'\n",
    "human_lr = 2.0\n",
    "\n",
    "train_split = 0.7\n",
    "train_size = int(n_demo * train_split)\n",
    "test_size = n_demo - train_size\n",
    "\n",
    "epochs = 100\n",
    "model_lr = 0.01\n",
    "\n",
    "data = generate_simulated_data(sim_policy, sim_time, n_demo, is_updating_internal_model, stochastic_human, human_lr, influence_type)\n",
    "robot_states, human_actions, human_obs, human_mental_states = data\n",
    "\n",
    "robot_states_train, robot_states_test = robot_states[:train_size], robot_states[train_size:]\n",
    "human_actions_train, human_actions_test = human_actions[:train_size], human_actions[train_size:]\n",
    "human_obs_train, human_obs_test = human_obs[:train_size], human_obs[train_size:]\n",
    "human_mental_states_train, human_mental_states_test = human_mental_states[:train_size], human_mental_states[train_size:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize simulated human trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2 if (n_demo < 9) else 3\n",
    "fig, axs = plt.subplots(n, n)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        ax = axs[i][j]\n",
    "        idx = (i*n) + j\n",
    "        human_traj = np.array(robot_states_train[idx]).squeeze()\n",
    "        ax.plot(human_traj[:,0],human_traj[:,1],'bo', markersize=3)\n",
    "        ax.axis(xmin=-1, xmax=1, ymin=-1, ymax=1)\n",
    "        ax.axis('equal')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize simulated human internal models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(n, n)\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        ax = axs[i][j]\n",
    "        idx = (i*n) + j\n",
    "        print(human_mental_states_train[idx])\n",
    "        human_internal_state_traj = np.array(human_mental_states_train[idx]).squeeze()\n",
    "        ax.plot(human_internal_state_traj, 'bo', markersize=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing trajectory data for estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_train = torch.tensor(robot_states_train, device=device)\n",
    "states_train = states_train.view(train_size * sim_time, nX)\n",
    "states_test = torch.tensor(robot_states_test, device=device)\n",
    "states_test = states_test.view(test_size * sim_time, nX)\n",
    "\n",
    "actions_train = torch.tensor(human_actions_train, device=device)\n",
    "actions_train = actions_train.view(train_size * sim_time, nU)\n",
    "actions_test = torch.tensor(human_actions_test, device=device)\n",
    "actions_test = actions_test.view(test_size * sim_time, nU)\n",
    "\n",
    "obs_train = torch.tensor(human_obs_train, device=device)\n",
    "obs_train = obs_train.view(train_size * sim_time, nX)\n",
    "obs_test = torch.tensor(human_obs_test, device=device)\n",
    "obs_test = obs_test.view(test_size * sim_time, nX)\n",
    "\n",
    "inputs_train = torch.cat((states_train, actions_train, obs_train), axis=1)\n",
    "inputs_train = inputs_train.view(train_size, sim_time, nX + nU + nX).double()\n",
    "inputs_test = torch.cat((states_test, actions_test, obs_test), axis=1)\n",
    "inputs_test = inputs_test.view(test_size, sim_time, nX + nU + nX).double()\n",
    "\n",
    "print('Train:', inputs_train.shape)\n",
    "print('Test:', inputs_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train $\\theta_H$ estimator E2E with LQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_estimator = ThetaEstimatorTransformer().to(device).double()\n",
    "optimizer = torch.optim.Adam(transformer_estimator.parameters(), lr=model_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(state, theta_H):\n",
    "    B_hat_tensor = theta_H * torch.tensor([[0., ],[1.0]], device = device).double()\n",
    "    P_hat = Riccati.apply(A_tensor, B_hat_tensor, Q_tensor, R_tensor)\n",
    "\n",
    "    K = torch.linalg.multi_dot((\n",
    "        torch.linalg.pinv(torch.add(\n",
    "            R_tensor,\n",
    "            torch.linalg.multi_dot((torch.transpose(B_hat_tensor, 0, 1), P_hat, B_hat_tensor))\n",
    "        )),\n",
    "        torch.transpose(B_hat_tensor, 0, 1),\n",
    "        P_hat,\n",
    "        A_tensor\n",
    "    ))\n",
    "    action_pred = -torch.matmul(K, state)\n",
    "    return action_pred\n",
    "\n",
    "def forward_pass(model, inputs, curr_traj_idx):\n",
    "    inputs = inputs.reshape(-1, inputs.shape[0], inputs.shape[1])\n",
    "    theta_Hs = model(inputs)\n",
    "    theta_Hs = theta_Hs.reshape(sim_time)\n",
    "    theta_H_error = 0\n",
    "\n",
    "    step_losses = []\n",
    "    for i in range(sim_time - 1):\n",
    "        # theta_H = theta_Hs[i] * 0.0 + human_mental_states[curr_traj_idx][i][0][0]\n",
    "        theta_H = theta_Hs[i]\n",
    "        theta_H_true = human_mental_states_train[curr_traj_idx][i][0][0]\n",
    "        theta_H_error += torch.linalg.norm(theta_H - theta_H_true).data.item()\n",
    "\n",
    "        input = inputs[0][i]\n",
    "        state, action, obs = torch.split(input, [2, 1, 2])\n",
    "        \n",
    "        action_pred = predict_action(state, theta_H)\n",
    "        # print(action_pred, action)\n",
    "\n",
    "        loss_fn = nn.MSELoss()\n",
    "        loss = loss_fn(action_pred, action)\n",
    "\n",
    "        if abs(loss) > 20:\n",
    "            ipdb.set_trace()\n",
    "\n",
    "        step_losses.append(loss)\n",
    "    \n",
    "    return step_losses, theta_H_error\n",
    "\n",
    "def train_epoch(model, inputs):\n",
    "    model.train()\n",
    "    all_losses = []\n",
    "    theta_H_error_all = 0\n",
    "    for idx in range(inputs.shape[0]):\n",
    "        inp = inputs[idx]\n",
    "        step_losses, theta_H_error = forward_pass(model, inp, idx)\n",
    "        theta_H_error_all += theta_H_error\n",
    "        all_losses += step_losses\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    total_loss = sum(all_losses) / (sim_time * inputs.shape[0])\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    theta_H_error_all /= (sim_time * inputs.shape[0])\n",
    "\n",
    "    return total_loss.data.item(), theta_H_error_all\n",
    "\n",
    "def test_epoch(model, inputs):\n",
    "    model.eval()\n",
    "    all_losses = []\n",
    "    theta_H_error_all = 0\n",
    "    for idx in range(inputs.shape[0]):\n",
    "        inp = inputs[idx]\n",
    "        step_losses, theta_H_error = forward_pass(model, inp, idx)\n",
    "        theta_H_error_all += theta_H_error\n",
    "        all_losses += step_losses\n",
    "    \n",
    "    total_loss = sum(all_losses) / (sim_time * inputs.shape[0])\n",
    "    theta_H_error_all /= (sim_time * inputs.shape[0])\n",
    "\n",
    "    return total_loss.data.item(), theta_H_error_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_list = range(1, epochs+1)\n",
    "train_losses, theta_H_train_errs, test_losses, theta_H_test_errs = [], [], [], []\n",
    "for epoch in tqdm(epoch_list):\n",
    "    # print(f'Epoch {epoch}: theta_H_error={theta_H_error}')\n",
    "    train_loss, theta_H_train_err = train_epoch(transformer_estimator, inputs_train)\n",
    "    test_loss, theta_H_test_err = test_epoch(transformer_estimator, inputs_test)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    theta_H_train_errs.append(theta_H_train_err)\n",
    "    theta_H_test_errs.append(theta_H_test_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_plot():\n",
    "    plt.plot(epoch_list, train_losses, color='b', label='Train Loss')\n",
    "    plt.plot(epoch_list, test_losses, color='r', label='Test Loss')\n",
    "    plt.title('Loss/MSE(u, u_pred)')\n",
    "    plt.legend()\n",
    "\n",
    "def error_plot():\n",
    "    plt.plot(epoch_list, theta_H_train_errs, color='b', label='Train Error')\n",
    "    plt.plot(epoch_list, theta_H_test_errs, color='r', label='Test Error')\n",
    "    plt.title('MSE(θH, θH_pred)')\n",
    "    plt.legend()\n",
    "\n",
    "loss_plot()\n",
    "plt.show()\n",
    "error_plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_name = 'varying_mental_varying_physical_large_eps'\n",
    "# exp_settings = {'sim_policy': sim_policy, 'sim_time': sim_time, 'n_demo': n_demo,\n",
    "#                 'is_updating_internal_model': is_updating_internal_model,\n",
    "#                 'stochastic_human': stochastic_human, 'human_lr': human_lr,\n",
    "#                 'train_split': train_split, 'epochs': epochs, 'model_lr': model_lr}\n",
    "\n",
    "# folder_name = 'experiments/{}'.format(exp_name)\n",
    "# # assert not os.path.isdir(folder_name)\n",
    "# os.makedirs(folder_name)\n",
    "\n",
    "# with open(folder_name + '/settings.json', 'w') as f:\n",
    "#     json.dump(exp_settings, f)\n",
    "\n",
    "# loss_plot()\n",
    "# plt.savefig(folder_name + '/loss_plot.png')\n",
    "# plt.clf()\n",
    "# error_plot()\n",
    "# plt.savefig(folder_name + '/error_plot.png')\n",
    "# plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL code similar to Assisted Perception paper\n",
    "# increase n_demo to get MSE under 0.1 (and try training for more epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(transformer_estimator, 'transformer_estimator.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ThetaEstimatorTransformer(\n",
       "  (transformer_encoder): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=5, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=5, bias=True)\n",
       "    (norm1): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.2, inplace=False)\n",
       "    (dropout2): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=12, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=12, out_features=8, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=8, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "transformer_estimator = torch.load('transformer_estimator.pt')\n",
    "transformer_estimator.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rollout function reward and environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import stable_baselines3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "rollout_reward_hist = []\n",
    "def rollout_reward(pi, robot_mode, influence_type):\n",
    "    sum_rewards = 0\n",
    "    NUM_ROLLOUTS = 10\n",
    "    for _ in range(NUM_ROLLOUTS):\n",
    "        env = HumanRobotEnv(robot_mode, 1.0, 'use_model_human', True, human_lr, influence_type)\n",
    "        env.set_environment(A, B, Q, R, None, None, 20)\n",
    "        env.set_action_set(None, u_t0_R_aug_set)\n",
    "        env.set_human_internal_model(transformer_estimator)\n",
    "        env.set_human_state(np.array([[0.4], [-0.0]]), np.array([[1.0]])) # Q: Should this be made varying?\n",
    "        obs = env.reset()\n",
    "        \n",
    "        for _ in range(env.episode_length):\n",
    "            if robot_mode == 'active_teaching':\n",
    "                action, _ = pi.predict(obs, deterministic=True)\n",
    "            else:\n",
    "                action = None\n",
    "            obs, _, done, _ = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "        sum_rewards += np.sum(env.current_demo_reward_traj)\n",
    "    sum_rewards /= NUM_ROLLOUTS\n",
    "    rollout_reward_hist.append(copy.deepcopy(sum_rewards))\n",
    "    return sum_rewards\n",
    "\n",
    "def rollout_env(pi, robot_mode, influence_type):\n",
    "    env = HumanRobotEnv(robot_mode, 1.0, 'use_model_human', True, human_lr, influence_type)\n",
    "    env.set_environment(A, B, Q, R, None, None, 20)\n",
    "    env.set_action_set(None, u_t0_R_aug_set)\n",
    "    env.set_human_internal_model(transformer_estimator)\n",
    "    env.set_human_state(np.array([[0.4], [-0.0]]), np.array([[1.0]])) # Q: Should this be made varying?\n",
    "    obs = env.reset()\n",
    "\n",
    "    for i in range(env.episode_length):\n",
    "        if robot_mode == 'active_teaching':\n",
    "            action, _ = pi.predict(obs, deterministic=True)\n",
    "        else:\n",
    "            action = None\n",
    "        obs, _, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "    return env\n",
    "\n",
    "class ModelSaveCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, log_dir, model_name, robot_action_mode, verbose):\n",
    "        super(ModelSaveCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, model_name)\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.robot_action_mode = robot_action_mode\n",
    "    \n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            mean_reward = rollout_reward(self.model, 'active_teaching', influence_type)\n",
    "            if self.verbose > 0:\n",
    "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "          \n",
    "            # New best model, you could save the agent here\n",
    "            if mean_reward > self.best_mean_reward:\n",
    "                self.best_mean_reward = mean_reward\n",
    "                # Example for saving best model\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"Saving new best model to {self.save_path}.zip\")\n",
    "                self.model.save(self.save_path)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_influence_policy(influence_type):\n",
    "    env = HumanRobotEnv('active_teaching', 1.0, 'use_model_human', True, human_lr, influence_type)\n",
    "    env.set_environment(A, B, Q, R, None, None, 20)\n",
    "    env.set_action_set(None, u_t0_R_aug_set)\n",
    "    env.set_human_internal_model(transformer_estimator)\n",
    "\n",
    "    env.set_human_state(np.array([[0.4], [0.]]), np.array([[1.0]]))\n",
    "    best_pi = None\n",
    "\n",
    "    log_dir = 'active_teaching/'\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    env = Monitor(env, log_dir)\n",
    "    pi_infl = PPO('MlpPolicy', env, verbose=True)\n",
    "    callback = ModelSaveCallback(check_freq=1000, log_dir=log_dir,\n",
    "                                model_name='active_teaching_easy',\n",
    "                                robot_action_mode=env.robot_action_mode,\n",
    "                                verbose=True)\n",
    "    pi_infl.learn(150000, callback=callback)\n",
    "    return pi_infl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arvind/Documents/closed-loop-influence/human.py:78: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  robot_states = torch.tensor(robot_states, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best mean reward: -inf - Last mean reward per episode: -9.50\n",
      "Saving new best model to active_teaching/active_teaching_easy.zip\n",
      "Best mean reward: -9.50 - Last mean reward per episode: -9.50\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19       |\n",
      "|    ep_rew_mean     | -9.23    |\n",
      "| time/              |          |\n",
      "|    fps             | 498      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Best mean reward: -9.50 - Last mean reward per episode: -6.05\n",
      "Saving new best model to active_teaching/active_teaching_easy.zip\n",
      "Best mean reward: -6.05 - Last mean reward per episode: -5.17\n",
      "Saving new best model to active_teaching/active_teaching_easy.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -9.23       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 505         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011118739 |\n",
      "|    clip_fraction        | 0.0425      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.61       |\n",
      "|    explained_variance   | 0.0673      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.194       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00225    |\n",
      "|    value_loss           | 3.88        |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.17 - Last mean reward per episode: -5.01\n",
      "Saving new best model to active_teaching/active_teaching_easy.zip\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.73\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -8.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 505         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013736146 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.1         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00685    |\n",
      "|    value_loss           | 0.396       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.73\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.65\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -8.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 496         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013070336 |\n",
      "|    clip_fraction        | 0.0887      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.289       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00813    |\n",
      "|    value_loss           | 0.48        |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.21\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.32\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 19         |\n",
      "|    ep_rew_mean          | -8.76      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 488        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 20         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01243511 |\n",
      "|    clip_fraction        | 0.0312     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.49      |\n",
      "|    explained_variance   | 0.917      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.263      |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0034    |\n",
      "|    value_loss           | 0.46       |\n",
      "----------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.22\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.88\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -8.57       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 492         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007887425 |\n",
      "|    clip_fraction        | 0.0563      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.152       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00602    |\n",
      "|    value_loss           | 0.45        |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.75\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.41\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -8.46        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 494          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 28           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063539036 |\n",
      "|    clip_fraction        | 0.0282       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.922        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.174        |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00433     |\n",
      "|    value_loss           | 0.407        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.45\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.68\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -8.27       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 497         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010006251 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.339       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00908    |\n",
      "|    value_loss           | 0.533       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.83\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.84\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -8.24       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 499         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006990855 |\n",
      "|    clip_fraction        | 0.0325      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.333       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00318    |\n",
      "|    value_loss           | 0.662       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.36\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.37\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -8.22       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 496         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007794453 |\n",
      "|    clip_fraction        | 0.0387      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.276       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00396    |\n",
      "|    value_loss           | 0.61        |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.70\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.70\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -7.82       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 494         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013603188 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.592       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    value_loss           | 0.808       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.35\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.67\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -7.67       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 495         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008511621 |\n",
      "|    clip_fraction        | 0.0489      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.795       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.414       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00493    |\n",
      "|    value_loss           | 0.896       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.16\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.13\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -7.26        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 497          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 53           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046903472 |\n",
      "|    clip_fraction        | 0.0216       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0.796        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.396        |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00236     |\n",
      "|    value_loss           | 0.854        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.45\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -7.24\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -7.35        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 498          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 57           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061441744 |\n",
      "|    clip_fraction        | 0.0364       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.17        |\n",
      "|    explained_variance   | 0.76         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.452        |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00376     |\n",
      "|    value_loss           | 0.905        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.77\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.04\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -7.24        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 500          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060795513 |\n",
      "|    clip_fraction        | 0.027        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.13        |\n",
      "|    explained_variance   | 0.787        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.319        |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0019      |\n",
      "|    value_loss           | 0.763        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.64\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.75\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -7.09        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 500          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 65           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061277347 |\n",
      "|    clip_fraction        | 0.0123       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.1         |\n",
      "|    explained_variance   | 0.755        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.369        |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.000692    |\n",
      "|    value_loss           | 0.851        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.02\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.38\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -7.16       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 501         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 69          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003168379 |\n",
      "|    clip_fraction        | 0.0199      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.713       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.46        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00192    |\n",
      "|    value_loss           | 0.989       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.22\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.89\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -7.48        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 501          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 73           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059447708 |\n",
      "|    clip_fraction        | 0.0368       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.744        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.348        |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00366     |\n",
      "|    value_loss           | 0.888        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.88\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.61\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.97       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 500         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 77          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006390891 |\n",
      "|    clip_fraction        | 0.0503      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.705       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.657       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00356    |\n",
      "|    value_loss           | 1.03        |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.86\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.92\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.97       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 499         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 82          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005108574 |\n",
      "|    clip_fraction        | 0.0373      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.748       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.28        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00411    |\n",
      "|    value_loss           | 0.837       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.50\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -4.84\n",
      "Saving new best model to active_teaching/active_teaching_easy.zip\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.58\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.61        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 496          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 86           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024804564 |\n",
      "|    clip_fraction        | 0.012        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.764        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.312        |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00192     |\n",
      "|    value_loss           | 0.771        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.82\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.96\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.99        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 497          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 90           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037498423 |\n",
      "|    clip_fraction        | 0.0338       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.955       |\n",
      "|    explained_variance   | 0.747        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.393        |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00227     |\n",
      "|    value_loss           | 0.802        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.85\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.52\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.94        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 497          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 94           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031192587 |\n",
      "|    clip_fraction        | 0.0155       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.97        |\n",
      "|    explained_variance   | 0.654        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.446        |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00117     |\n",
      "|    value_loss           | 1.02         |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.00\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.86\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.53       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 497         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 98          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002796034 |\n",
      "|    clip_fraction        | 0.0147      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.949      |\n",
      "|    explained_variance   | 0.727       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.333       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00114    |\n",
      "|    value_loss           | 0.899       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.98\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.49\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.81        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 497          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 102          |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034631118 |\n",
      "|    clip_fraction        | 0.0189       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.915       |\n",
      "|    explained_variance   | 0.735        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.291        |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00216     |\n",
      "|    value_loss           | 0.805        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.97\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.92\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.49       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 496         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 107         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002987603 |\n",
      "|    clip_fraction        | 0.0307      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.874      |\n",
      "|    explained_variance   | 0.802       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.267       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00248    |\n",
      "|    value_loss           | 0.626       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.62\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.90\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.72        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 496          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 111          |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036897687 |\n",
      "|    clip_fraction        | 0.0132       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.837       |\n",
      "|    explained_variance   | 0.724        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.447        |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00199     |\n",
      "|    value_loss           | 0.824        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.70\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.70\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.52        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 496          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 115          |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048883297 |\n",
      "|    clip_fraction        | 0.0436       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.928       |\n",
      "|    explained_variance   | 0.767        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.297        |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.0044      |\n",
      "|    value_loss           | 0.685        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.03\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.06\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.13        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 496          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 119          |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045950525 |\n",
      "|    clip_fraction        | 0.0169       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.868       |\n",
      "|    explained_variance   | 0.718        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.358        |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00138     |\n",
      "|    value_loss           | 0.794        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.38\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.12\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.44       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 496         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 123         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004911077 |\n",
      "|    clip_fraction        | 0.0493      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.766      |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.205       |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00403    |\n",
      "|    value_loss           | 0.685       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.66\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.08\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.62        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 497          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 127          |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048441812 |\n",
      "|    clip_fraction        | 0.0528       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.776       |\n",
      "|    explained_variance   | 0.734        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.273        |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.0067      |\n",
      "|    value_loss           | 0.729        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.35\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.76\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.17        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 497          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 131          |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017848965 |\n",
      "|    clip_fraction        | 0.0165       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.824       |\n",
      "|    explained_variance   | 0.798        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.194        |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00156     |\n",
      "|    value_loss           | 0.587        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.88\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.19\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 498          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 135          |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026222644 |\n",
      "|    clip_fraction        | 0.017        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.772       |\n",
      "|    explained_variance   | 0.749        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.373        |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.000242    |\n",
      "|    value_loss           | 0.691        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.40\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.14\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.37       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 498         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 139         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003759991 |\n",
      "|    clip_fraction        | 0.0324      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.791      |\n",
      "|    explained_variance   | 0.778       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.348       |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00253    |\n",
      "|    value_loss           | 0.651       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.25\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.38\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.56        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 499          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 143          |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035652504 |\n",
      "|    clip_fraction        | 0.0267       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.792       |\n",
      "|    explained_variance   | 0.855        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.227        |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00291     |\n",
      "|    value_loss           | 0.428        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.30\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.01\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.04        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 500          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 147          |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031931628 |\n",
      "|    clip_fraction        | 0.0113       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.829       |\n",
      "|    explained_variance   | 0.755        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.271        |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.00129     |\n",
      "|    value_loss           | 0.615        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.27\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.75\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 499          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 151          |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033533927 |\n",
      "|    clip_fraction        | 0.0304       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.793       |\n",
      "|    explained_variance   | 0.755        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.374        |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00299     |\n",
      "|    value_loss           | 0.657        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -4.99\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -4.92\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.19        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 499          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 155          |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013136013 |\n",
      "|    clip_fraction        | 0.0175       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.819       |\n",
      "|    explained_variance   | 0.788        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.353        |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.000967    |\n",
      "|    value_loss           | 0.62         |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.46\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.78\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.39       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 500         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 159         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003666421 |\n",
      "|    clip_fraction        | 0.0134      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.767      |\n",
      "|    explained_variance   | 0.781       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.254       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00156    |\n",
      "|    value_loss           | 0.611       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.27\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.40\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 500          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 163          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011611274 |\n",
      "|    clip_fraction        | 0.0177       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.746       |\n",
      "|    explained_variance   | 0.804        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.367        |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00114     |\n",
      "|    value_loss           | 0.541        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.49\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.17\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.29        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 501          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 167          |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033228933 |\n",
      "|    clip_fraction        | 0.0187       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.721       |\n",
      "|    explained_variance   | 0.827        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.267        |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.00195     |\n",
      "|    value_loss           | 0.496        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.73\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.67\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.89\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.32        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 500          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 171          |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028177479 |\n",
      "|    clip_fraction        | 0.0306       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.757       |\n",
      "|    explained_variance   | 0.828        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.23         |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00229     |\n",
      "|    value_loss           | 0.477        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.95\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.46\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.28        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 501          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 175          |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016347875 |\n",
      "|    clip_fraction        | 0.0143       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.774       |\n",
      "|    explained_variance   | 0.819        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.235        |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00148     |\n",
      "|    value_loss           | 0.498        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.33\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.77\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.34        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 501          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 179          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030769543 |\n",
      "|    clip_fraction        | 0.0163       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.772       |\n",
      "|    explained_variance   | 0.83         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.297        |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00158     |\n",
      "|    value_loss           | 0.481        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.19\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.32\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.33        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 501          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 183          |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023948236 |\n",
      "|    clip_fraction        | 0.00518      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.801       |\n",
      "|    explained_variance   | 0.734        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.381        |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.000748    |\n",
      "|    value_loss           | 0.67         |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.46\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.40\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 501         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 187         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001967885 |\n",
      "|    clip_fraction        | 0.0263      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.78       |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.183       |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00135    |\n",
      "|    value_loss           | 0.514       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.66\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.33\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.51        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 501          |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 191          |\n",
      "|    total_timesteps      | 96256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025970845 |\n",
      "|    clip_fraction        | 0.00654      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.801       |\n",
      "|    explained_variance   | 0.803        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.42         |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | 6.63e-05     |\n",
      "|    value_loss           | 0.555        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.84\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.01\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.35       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 502         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 195         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004668733 |\n",
      "|    clip_fraction        | 0.0324      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.867      |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.288       |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00289    |\n",
      "|    value_loss           | 0.593       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.97\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.10\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 19         |\n",
      "|    ep_rew_mean          | -6.33      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 501        |\n",
      "|    iterations           | 49         |\n",
      "|    time_elapsed         | 199        |\n",
      "|    total_timesteps      | 100352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00473419 |\n",
      "|    clip_fraction        | 0.0483     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.785     |\n",
      "|    explained_variance   | 0.765      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.311      |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.00402   |\n",
      "|    value_loss           | 0.673      |\n",
      "----------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.70\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.30\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.51       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 502         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 203         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003305681 |\n",
      "|    clip_fraction        | 0.0185      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.775      |\n",
      "|    explained_variance   | 0.854       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.209       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.000765   |\n",
      "|    value_loss           | 0.413       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.17\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.58\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.27        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 502          |\n",
      "|    iterations           | 51           |\n",
      "|    time_elapsed         | 207          |\n",
      "|    total_timesteps      | 104448       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038238103 |\n",
      "|    clip_fraction        | 0.0344       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.786       |\n",
      "|    explained_variance   | 0.783        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.253        |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.00227     |\n",
      "|    value_loss           | 0.584        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.29\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.04\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.19        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 502          |\n",
      "|    iterations           | 52           |\n",
      "|    time_elapsed         | 211          |\n",
      "|    total_timesteps      | 106496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042302334 |\n",
      "|    clip_fraction        | 0.0339       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.771       |\n",
      "|    explained_variance   | 0.8          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.253        |\n",
      "|    n_updates            | 510          |\n",
      "|    policy_gradient_loss | -0.00285     |\n",
      "|    value_loss           | 0.558        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.37\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.93\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.23        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 502          |\n",
      "|    iterations           | 53           |\n",
      "|    time_elapsed         | 215          |\n",
      "|    total_timesteps      | 108544       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039564837 |\n",
      "|    clip_fraction        | 0.0204       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.789       |\n",
      "|    explained_variance   | 0.821        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.197        |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.0019      |\n",
      "|    value_loss           | 0.498        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.94\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.10\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.39        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 503          |\n",
      "|    iterations           | 54           |\n",
      "|    time_elapsed         | 219          |\n",
      "|    total_timesteps      | 110592       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035134726 |\n",
      "|    clip_fraction        | 0.0163       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.773       |\n",
      "|    explained_variance   | 0.795        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.223        |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.00199     |\n",
      "|    value_loss           | 0.526        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.77\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.74\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.26        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 502          |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 223          |\n",
      "|    total_timesteps      | 112640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042951065 |\n",
      "|    clip_fraction        | 0.0131       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.79        |\n",
      "|    explained_variance   | 0.787        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.392        |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.00149     |\n",
      "|    value_loss           | 0.619        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.40\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.09\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.27        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 503          |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 227          |\n",
      "|    total_timesteps      | 114688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021162843 |\n",
      "|    clip_fraction        | 0.0126       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.76        |\n",
      "|    explained_variance   | 0.803        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.22         |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.00109     |\n",
      "|    value_loss           | 0.576        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.16\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.15\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 19         |\n",
      "|    ep_rew_mean          | -6.22      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 503        |\n",
      "|    iterations           | 57         |\n",
      "|    time_elapsed         | 232        |\n",
      "|    total_timesteps      | 116736     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00400229 |\n",
      "|    clip_fraction        | 0.0302     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.788     |\n",
      "|    explained_variance   | 0.745      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.384      |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.00245   |\n",
      "|    value_loss           | 0.679      |\n",
      "----------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.67\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.27\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.18        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 503          |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 235          |\n",
      "|    total_timesteps      | 118784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031960506 |\n",
      "|    clip_fraction        | 0.0267       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.76        |\n",
      "|    explained_variance   | 0.777        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.258        |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.00263     |\n",
      "|    value_loss           | 0.602        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.21\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.23\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.36       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 503         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 240         |\n",
      "|    total_timesteps      | 120832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004281707 |\n",
      "|    clip_fraction        | 0.0294      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.723      |\n",
      "|    explained_variance   | 0.761       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.266       |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.00198    |\n",
      "|    value_loss           | 0.603       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.71\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.27\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 19         |\n",
      "|    ep_rew_mean          | -6.45      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 503        |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 243        |\n",
      "|    total_timesteps      | 122880     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00474695 |\n",
      "|    clip_fraction        | 0.037      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.771     |\n",
      "|    explained_variance   | 0.773      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.338      |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | -0.00262   |\n",
      "|    value_loss           | 0.643      |\n",
      "----------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.71\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.14        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 503          |\n",
      "|    iterations           | 61           |\n",
      "|    time_elapsed         | 247          |\n",
      "|    total_timesteps      | 124928       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013754882 |\n",
      "|    clip_fraction        | 0.0133       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.764       |\n",
      "|    explained_variance   | 0.822        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.208        |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.000165    |\n",
      "|    value_loss           | 0.538        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.99\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.38\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.12       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 504         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 251         |\n",
      "|    total_timesteps      | 126976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004238611 |\n",
      "|    clip_fraction        | 0.0515      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.741      |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.198       |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0065     |\n",
      "|    value_loss           | 0.513       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.17\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.12\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.97\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.02        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 503          |\n",
      "|    iterations           | 63           |\n",
      "|    time_elapsed         | 256          |\n",
      "|    total_timesteps      | 129024       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040837876 |\n",
      "|    clip_fraction        | 0.0272       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.74        |\n",
      "|    explained_variance   | 0.804        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.205        |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.00323     |\n",
      "|    value_loss           | 0.526        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.14\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.79\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.24        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 503          |\n",
      "|    iterations           | 64           |\n",
      "|    time_elapsed         | 260          |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026535618 |\n",
      "|    clip_fraction        | 0.0291       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.692       |\n",
      "|    explained_variance   | 0.814        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.205        |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.0024      |\n",
      "|    value_loss           | 0.46         |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.89\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.83\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.35        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 503          |\n",
      "|    iterations           | 65           |\n",
      "|    time_elapsed         | 264          |\n",
      "|    total_timesteps      | 133120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056760604 |\n",
      "|    clip_fraction        | 0.0203       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.697       |\n",
      "|    explained_variance   | 0.827        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.163        |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.00229     |\n",
      "|    value_loss           | 0.467        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.83\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.12\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.02       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 503         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 268         |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003727631 |\n",
      "|    clip_fraction        | 0.022       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.74       |\n",
      "|    explained_variance   | 0.826       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.127       |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.00328    |\n",
      "|    value_loss           | 0.48        |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.03\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.80\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.22       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 503         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 272         |\n",
      "|    total_timesteps      | 137216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004216915 |\n",
      "|    clip_fraction        | 0.0367      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.715      |\n",
      "|    explained_variance   | 0.825       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.329       |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.00292    |\n",
      "|    value_loss           | 0.45        |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.19\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -5.96       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 503         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 276         |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004338691 |\n",
      "|    clip_fraction        | 0.0323      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.75       |\n",
      "|    explained_variance   | 0.789       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.291       |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.0032     |\n",
      "|    value_loss           | 0.509       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.97\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.71\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.43       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 502         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 281         |\n",
      "|    total_timesteps      | 141312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002650167 |\n",
      "|    clip_fraction        | 0.034       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.719      |\n",
      "|    explained_variance   | 0.814       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.21        |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0024     |\n",
      "|    value_loss           | 0.393       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.44\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.43\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.61        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 501          |\n",
      "|    iterations           | 70           |\n",
      "|    time_elapsed         | 285          |\n",
      "|    total_timesteps      | 143360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039062484 |\n",
      "|    clip_fraction        | 0.0163       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.758       |\n",
      "|    explained_variance   | 0.802        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.105        |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -0.00181     |\n",
      "|    value_loss           | 0.519        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -6.04\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.38\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.22        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 501          |\n",
      "|    iterations           | 71           |\n",
      "|    time_elapsed         | 290          |\n",
      "|    total_timesteps      | 145408       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063469443 |\n",
      "|    clip_fraction        | 0.0599       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.759       |\n",
      "|    explained_variance   | 0.793        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.269        |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.00529     |\n",
      "|    value_loss           | 0.609        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -5.46\n",
      "Best mean reward: -4.84 - Last mean reward per episode: -4.81\n",
      "Saving new best model to active_teaching/active_teaching_easy.zip\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.37        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 500          |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 294          |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063016345 |\n",
      "|    clip_fraction        | 0.0456       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.714       |\n",
      "|    explained_variance   | 0.776        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.278        |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.00334     |\n",
      "|    value_loss           | 0.589        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.81 - Last mean reward per episode: -6.00\n",
      "Best mean reward: -4.81 - Last mean reward per episode: -6.01\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.16        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 499          |\n",
      "|    iterations           | 73           |\n",
      "|    time_elapsed         | 299          |\n",
      "|    total_timesteps      | 149504       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021151328 |\n",
      "|    clip_fraction        | 0.0216       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.731       |\n",
      "|    explained_variance   | 0.821        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.312        |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.00231     |\n",
      "|    value_loss           | 0.489        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.81 - Last mean reward per episode: -6.27\n",
      "Best mean reward: -4.81 - Last mean reward per episode: -5.09\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.31        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 488          |\n",
      "|    iterations           | 74           |\n",
      "|    time_elapsed         | 309          |\n",
      "|    total_timesteps      | 151552       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026578177 |\n",
      "|    clip_fraction        | 0.00879      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.719       |\n",
      "|    explained_variance   | 0.809        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.297        |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -0.000387    |\n",
      "|    value_loss           | 0.506        |\n",
      "------------------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Best mean reward: -inf - Last mean reward per episode: -9.42\n",
      "Saving new best model to active_teaching/active_teaching_easy.zip\n",
      "Best mean reward: -9.42 - Last mean reward per episode: -9.42\n",
      "Saving new best model to active_teaching/active_teaching_easy.zip\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19       |\n",
      "|    ep_rew_mean     | -9.24    |\n",
      "| time/              |          |\n",
      "|    fps             | 560      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Best mean reward: -9.42 - Last mean reward per episode: -6.09\n",
      "Saving new best model to active_teaching/active_teaching_easy.zip\n",
      "Best mean reward: -6.09 - Last mean reward per episode: -5.23\n",
      "Saving new best model to active_teaching/active_teaching_easy.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -9.09       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 541         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007968228 |\n",
      "|    clip_fraction        | 0.0338      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.61       |\n",
      "|    explained_variance   | 0.0361      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.163       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00331    |\n",
      "|    value_loss           | 2.92        |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -5.98\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -5.81\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -9.17        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 534          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075941286 |\n",
      "|    clip_fraction        | 0.0169       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.59        |\n",
      "|    explained_variance   | 0.81         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.124        |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00196     |\n",
      "|    value_loss           | 0.427        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -6.31\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -5.69\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -8.97       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 525         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010793617 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.174       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    value_loss           | 0.358       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -5.69\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -6.40\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -8.93       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 522         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009370433 |\n",
      "|    clip_fraction        | 0.0443      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.176       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00627    |\n",
      "|    value_loss           | 0.383       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -7.04\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -6.47\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -8.79       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 519         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013566293 |\n",
      "|    clip_fraction        | 0.0991      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.147       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00931    |\n",
      "|    value_loss           | 0.337       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -6.35\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -7.15\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -8.74       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 515         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010754185 |\n",
      "|    clip_fraction        | 0.0426      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.137       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00391    |\n",
      "|    value_loss           | 0.342       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -6.33\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -5.64\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -8.03       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 513         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009748157 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.102       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00956    |\n",
      "|    value_loss           | 0.378       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -5.60\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -5.83\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -7.83       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 509         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014496352 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.888       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.235       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    value_loss           | 0.519       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -6.47\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -5.75\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -7.35       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 509         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010297379 |\n",
      "|    clip_fraction        | 0.0863      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.253       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    value_loss           | 0.561       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -6.32\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -6.81\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -7.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 510         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 44          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008522492 |\n",
      "|    clip_fraction        | 0.0804      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.808       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.377       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    value_loss           | 0.733       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -6.06\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -6.30\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.65        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 511          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 48           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048651663 |\n",
      "|    clip_fraction        | 0.0429       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.964       |\n",
      "|    explained_variance   | 0.798        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.402        |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.0063      |\n",
      "|    value_loss           | 0.735        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -5.24\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -7.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.83       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 512         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006629058 |\n",
      "|    clip_fraction        | 0.0685      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.805      |\n",
      "|    explained_variance   | 0.807       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.31        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00768    |\n",
      "|    value_loss           | 0.572       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -5.90\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -5.65\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.59        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 513          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045651463 |\n",
      "|    clip_fraction        | 0.0609       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.723       |\n",
      "|    explained_variance   | 0.835        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.249        |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00483     |\n",
      "|    value_loss           | 0.531        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -6.89\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -5.36\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.31        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 513          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 59           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023553027 |\n",
      "|    clip_fraction        | 0.0225       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.611       |\n",
      "|    explained_variance   | 0.843        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.146        |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00299     |\n",
      "|    value_loss           | 0.457        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -6.66\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -5.88\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.18        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 514          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 63           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023546512 |\n",
      "|    clip_fraction        | 0.0254       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.507       |\n",
      "|    explained_variance   | 0.881        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.128        |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00245     |\n",
      "|    value_loss           | 0.351        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -5.80\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -6.96\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.28        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 514          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 67           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030478719 |\n",
      "|    clip_fraction        | 0.0309       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.47        |\n",
      "|    explained_variance   | 0.891        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.118        |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0032      |\n",
      "|    value_loss           | 0.317        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -6.01\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -5.78\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.44        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 515          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016771948 |\n",
      "|    clip_fraction        | 0.0248       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.42        |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.21         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00221     |\n",
      "|    value_loss           | 0.311        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -5.29\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -5.40\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -5.98        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 515          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 75           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023936534 |\n",
      "|    clip_fraction        | 0.0349       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.388       |\n",
      "|    explained_variance   | 0.932        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.109        |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00372     |\n",
      "|    value_loss           | 0.226        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.23 - Last mean reward per episode: -5.01\n",
      "Saving new best model to active_teaching/active_teaching_easy.zip\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.60\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -5.85       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 515         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 79          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002656779 |\n",
      "|    clip_fraction        | 0.0277      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.363      |\n",
      "|    explained_variance   | 0.898       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.114       |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00203    |\n",
      "|    value_loss           | 0.277       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.61\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.03\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.40\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -5.82        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 514          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 83           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010083301 |\n",
      "|    clip_fraction        | 0.0157       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.316       |\n",
      "|    explained_variance   | 0.918        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.111        |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    value_loss           | 0.219        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.25\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.16\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -5.95       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 514         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 87          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001414528 |\n",
      "|    clip_fraction        | 0.0197      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.316      |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.148       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.000589   |\n",
      "|    value_loss           | 0.182       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.33\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.61\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.14        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 514          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 91           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012332685 |\n",
      "|    clip_fraction        | 0.013        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.308       |\n",
      "|    explained_variance   | 0.936        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0752       |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.000706    |\n",
      "|    value_loss           | 0.176        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.44\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.45\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 514          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 95           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011186053 |\n",
      "|    clip_fraction        | 0.0245       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.29        |\n",
      "|    explained_variance   | 0.943        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0849       |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.0015      |\n",
      "|    value_loss           | 0.166        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.12\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.37\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.15        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 513          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 99           |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008117048 |\n",
      "|    clip_fraction        | 0.0126       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.281       |\n",
      "|    explained_variance   | 0.942        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0652       |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.000635    |\n",
      "|    value_loss           | 0.164        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.31\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -7.25\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.23        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 513          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 103          |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009963936 |\n",
      "|    clip_fraction        | 0.014        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.297       |\n",
      "|    explained_variance   | 0.953        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0793       |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00188     |\n",
      "|    value_loss           | 0.141        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.03\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.17\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 19         |\n",
      "|    ep_rew_mean          | -6.26      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 513        |\n",
      "|    iterations           | 27         |\n",
      "|    time_elapsed         | 107        |\n",
      "|    total_timesteps      | 55296      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00301542 |\n",
      "|    clip_fraction        | 0.0285     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.317     |\n",
      "|    explained_variance   | 0.949      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0993     |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.00466   |\n",
      "|    value_loss           | 0.148      |\n",
      "----------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.53\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.71\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.01        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 513          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 111          |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012377625 |\n",
      "|    clip_fraction        | 0.0154       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.311       |\n",
      "|    explained_variance   | 0.952        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0753       |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.000372    |\n",
      "|    value_loss           | 0.138        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.05\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.48\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.04        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 513          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 115          |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013261895 |\n",
      "|    clip_fraction        | 0.00923      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.292       |\n",
      "|    explained_variance   | 0.956        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.113        |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.000667    |\n",
      "|    value_loss           | 0.122        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.06\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.11\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -5.94        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 513          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 119          |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011270959 |\n",
      "|    clip_fraction        | 0.00869      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.231       |\n",
      "|    explained_variance   | 0.949        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.115        |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | 6.46e-05     |\n",
      "|    value_loss           | 0.15         |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.99\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.98\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -5.95       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 514         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 123         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003342065 |\n",
      "|    clip_fraction        | 0.0244      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.239      |\n",
      "|    explained_variance   | 0.944       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0388      |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00199    |\n",
      "|    value_loss           | 0.144       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.87\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -7.33\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -5.92        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 514          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 127          |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013268227 |\n",
      "|    clip_fraction        | 0.0229       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.254       |\n",
      "|    explained_variance   | 0.943        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.13         |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00275     |\n",
      "|    value_loss           | 0.156        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.98\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.14\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.36        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 512          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 131          |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010808978 |\n",
      "|    clip_fraction        | 0.0124       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.219       |\n",
      "|    explained_variance   | 0.952        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0668       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.000185    |\n",
      "|    value_loss           | 0.126        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.67\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.73\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -5.97        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 512          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 135          |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013486209 |\n",
      "|    clip_fraction        | 0.0148       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.232       |\n",
      "|    explained_variance   | 0.967        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0705       |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.000348    |\n",
      "|    value_loss           | 0.102        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.60\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.15\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.02        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 512          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 139          |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012190335 |\n",
      "|    clip_fraction        | 0.0132       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.227       |\n",
      "|    explained_variance   | 0.947        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0466       |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00114     |\n",
      "|    value_loss           | 0.13         |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.77\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.66\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -5.98        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 513          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 143          |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018277547 |\n",
      "|    clip_fraction        | 0.0149       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.251       |\n",
      "|    explained_variance   | 0.961        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.063        |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.001       |\n",
      "|    value_loss           | 0.113        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.65\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.76\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.23       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 513         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 147         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001592241 |\n",
      "|    clip_fraction        | 0.0322      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.225      |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0323      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00369    |\n",
      "|    value_loss           | 0.0967      |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.43\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.97\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -5.93        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 513          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 151          |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013240908 |\n",
      "|    clip_fraction        | 0.0168       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.259       |\n",
      "|    explained_variance   | 0.96         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0462       |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.000408    |\n",
      "|    value_loss           | 0.127        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.61\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.62\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -5.73        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 513          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 155          |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014926842 |\n",
      "|    clip_fraction        | 0.0131       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.242       |\n",
      "|    explained_variance   | 0.962        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0327       |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -1.09e-05    |\n",
      "|    value_loss           | 0.111        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.90\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.72\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.03        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 513          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 159          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009598434 |\n",
      "|    clip_fraction        | 0.0128       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.188       |\n",
      "|    explained_variance   | 0.944        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.06         |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00147     |\n",
      "|    value_loss           | 0.141        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.02\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.59\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 19            |\n",
      "|    ep_rew_mean          | -5.84         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 513           |\n",
      "|    iterations           | 41            |\n",
      "|    time_elapsed         | 163           |\n",
      "|    total_timesteps      | 83968         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00073488127 |\n",
      "|    clip_fraction        | 0.01          |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.205        |\n",
      "|    explained_variance   | 0.964         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0615        |\n",
      "|    n_updates            | 400           |\n",
      "|    policy_gradient_loss | 0.000198      |\n",
      "|    value_loss           | 0.1           |\n",
      "-------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.31\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.65\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.48\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -5.79        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 512          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 167          |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011132966 |\n",
      "|    clip_fraction        | 0.014        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.221       |\n",
      "|    explained_variance   | 0.958        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0812       |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.000241    |\n",
      "|    value_loss           | 0.113        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.85\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.63\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 19            |\n",
      "|    ep_rew_mean          | -6.13         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 512           |\n",
      "|    iterations           | 43            |\n",
      "|    time_elapsed         | 171           |\n",
      "|    total_timesteps      | 88064         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00069214194 |\n",
      "|    clip_fraction        | 0.0123        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.209        |\n",
      "|    explained_variance   | 0.963         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0697        |\n",
      "|    n_updates            | 420           |\n",
      "|    policy_gradient_loss | -0.00044      |\n",
      "|    value_loss           | 0.101         |\n",
      "-------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.92\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.34\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -5.79        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 512          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 175          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010832986 |\n",
      "|    clip_fraction        | 0.0175       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.239       |\n",
      "|    explained_variance   | 0.96         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.106        |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.0016      |\n",
      "|    value_loss           | 0.119        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.40\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.87\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -5.79        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 512          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 179          |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017289671 |\n",
      "|    clip_fraction        | 0.0207       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.179       |\n",
      "|    explained_variance   | 0.947        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0563       |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00107     |\n",
      "|    value_loss           | 0.145        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.06\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.69\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.06        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 511          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 184          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010541984 |\n",
      "|    clip_fraction        | 0.0103       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.207       |\n",
      "|    explained_variance   | 0.96         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.109        |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.000103    |\n",
      "|    value_loss           | 0.1          |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.85\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.38\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.06        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 511          |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 188          |\n",
      "|    total_timesteps      | 96256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009069489 |\n",
      "|    clip_fraction        | 0.0145       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.216       |\n",
      "|    explained_variance   | 0.95         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0933       |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.000745    |\n",
      "|    value_loss           | 0.141        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.87\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.84\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.27        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 511          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 192          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010075917 |\n",
      "|    clip_fraction        | 0.0145       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.22        |\n",
      "|    explained_variance   | 0.974        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0541       |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.00111     |\n",
      "|    value_loss           | 0.0704       |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.29\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.20\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.04        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 511          |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 196          |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026202206 |\n",
      "|    clip_fraction        | 0.0281       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.216       |\n",
      "|    explained_variance   | 0.952        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.061        |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.00233     |\n",
      "|    value_loss           | 0.143        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.52\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.49\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 511          |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 200          |\n",
      "|    total_timesteps      | 102400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004944216 |\n",
      "|    clip_fraction        | 0.00771      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.208       |\n",
      "|    explained_variance   | 0.973        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0342       |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | 0.000106     |\n",
      "|    value_loss           | 0.071        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.10\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.06\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -5.92       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 511         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 204         |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000693721 |\n",
      "|    clip_fraction        | 0.016       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.252      |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0539      |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.00077    |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.64\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.67\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -5.82        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 509          |\n",
      "|    iterations           | 52           |\n",
      "|    time_elapsed         | 208          |\n",
      "|    total_timesteps      | 106496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013998176 |\n",
      "|    clip_fraction        | 0.0126       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.233       |\n",
      "|    explained_variance   | 0.971        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0539       |\n",
      "|    n_updates            | 510          |\n",
      "|    policy_gradient_loss | -1.76e-05    |\n",
      "|    value_loss           | 0.0794       |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.59\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.53\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.02        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 509          |\n",
      "|    iterations           | 53           |\n",
      "|    time_elapsed         | 212          |\n",
      "|    total_timesteps      | 108544       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012891248 |\n",
      "|    clip_fraction        | 0.0243       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.195       |\n",
      "|    explained_variance   | 0.964        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0384       |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.00165     |\n",
      "|    value_loss           | 0.0938       |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.61\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.30\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 509          |\n",
      "|    iterations           | 54           |\n",
      "|    time_elapsed         | 216          |\n",
      "|    total_timesteps      | 110592       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023246948 |\n",
      "|    clip_fraction        | 0.023        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.21        |\n",
      "|    explained_variance   | 0.967        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0869       |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.00144     |\n",
      "|    value_loss           | 0.0935       |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -7.16\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.64\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -5.98        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 509          |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 220          |\n",
      "|    total_timesteps      | 112640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011365381 |\n",
      "|    clip_fraction        | 0.0167       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.211       |\n",
      "|    explained_variance   | 0.966        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0786       |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.000722    |\n",
      "|    value_loss           | 0.0936       |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.85\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.03\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.27        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 509          |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 224          |\n",
      "|    total_timesteps      | 114688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016572813 |\n",
      "|    clip_fraction        | 0.0154       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.204       |\n",
      "|    explained_variance   | 0.955        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0593       |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.00081     |\n",
      "|    value_loss           | 0.128        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.09\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.03\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.17        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 509          |\n",
      "|    iterations           | 57           |\n",
      "|    time_elapsed         | 229          |\n",
      "|    total_timesteps      | 116736       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013874017 |\n",
      "|    clip_fraction        | 0.0174       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.24        |\n",
      "|    explained_variance   | 0.974        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0391       |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.00125     |\n",
      "|    value_loss           | 0.0776       |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.09\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.18\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.16        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 509          |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 233          |\n",
      "|    total_timesteps      | 118784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011378962 |\n",
      "|    clip_fraction        | 0.0171       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.233       |\n",
      "|    explained_variance   | 0.963        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0428       |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.0019      |\n",
      "|    value_loss           | 0.112        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.08\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.80\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -5.69        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 509          |\n",
      "|    iterations           | 59           |\n",
      "|    time_elapsed         | 237          |\n",
      "|    total_timesteps      | 120832       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028858758 |\n",
      "|    clip_fraction        | 0.027        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.24        |\n",
      "|    explained_variance   | 0.968        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0396       |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.000869    |\n",
      "|    value_loss           | 0.0951       |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.38\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.62\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.01        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 509          |\n",
      "|    iterations           | 60           |\n",
      "|    time_elapsed         | 241          |\n",
      "|    total_timesteps      | 122880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015649495 |\n",
      "|    clip_fraction        | 0.0218       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.223       |\n",
      "|    explained_variance   | 0.953        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0388       |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    value_loss           | 0.123        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.10\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.67\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -5.89       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 509         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 245         |\n",
      "|    total_timesteps      | 124928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001350469 |\n",
      "|    clip_fraction        | 0.0183      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.257      |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0563      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.000662   |\n",
      "|    value_loss           | 0.121       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.55\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.56\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -5.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 509          |\n",
      "|    iterations           | 62           |\n",
      "|    time_elapsed         | 249          |\n",
      "|    total_timesteps      | 126976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011412862 |\n",
      "|    clip_fraction        | 0.0127       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.213       |\n",
      "|    explained_variance   | 0.959        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0655       |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.000644    |\n",
      "|    value_loss           | 0.116        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.68\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.02\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.99\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -5.97        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 509          |\n",
      "|    iterations           | 63           |\n",
      "|    time_elapsed         | 253          |\n",
      "|    total_timesteps      | 129024       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009479311 |\n",
      "|    clip_fraction        | 0.00869      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.213       |\n",
      "|    explained_variance   | 0.969        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0338       |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | 0.000111     |\n",
      "|    value_loss           | 0.0825       |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.19\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.64\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -5.72        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 509          |\n",
      "|    iterations           | 64           |\n",
      "|    time_elapsed         | 257          |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013571449 |\n",
      "|    clip_fraction        | 0.019        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.2         |\n",
      "|    explained_variance   | 0.96         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.086        |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    value_loss           | 0.111        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.39\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.09\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -5.98        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 509          |\n",
      "|    iterations           | 65           |\n",
      "|    time_elapsed         | 261          |\n",
      "|    total_timesteps      | 133120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016090996 |\n",
      "|    clip_fraction        | 0.0119       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.192       |\n",
      "|    explained_variance   | 0.966        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0629       |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.000552    |\n",
      "|    value_loss           | 0.0892       |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.02\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.21\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.03        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 509          |\n",
      "|    iterations           | 66           |\n",
      "|    time_elapsed         | 265          |\n",
      "|    total_timesteps      | 135168       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011491913 |\n",
      "|    clip_fraction        | 0.0133       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.206       |\n",
      "|    explained_variance   | 0.966        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0392       |\n",
      "|    n_updates            | 650          |\n",
      "|    policy_gradient_loss | -0.00105     |\n",
      "|    value_loss           | 0.0901       |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.95\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.45\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -5.65        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 509          |\n",
      "|    iterations           | 67           |\n",
      "|    time_elapsed         | 269          |\n",
      "|    total_timesteps      | 137216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007976664 |\n",
      "|    clip_fraction        | 0.0101       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.203       |\n",
      "|    explained_variance   | 0.971        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0455       |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.000303    |\n",
      "|    value_loss           | 0.0854       |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.48\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.32\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.28        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 509          |\n",
      "|    iterations           | 68           |\n",
      "|    time_elapsed         | 273          |\n",
      "|    total_timesteps      | 139264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013337961 |\n",
      "|    clip_fraction        | 0.0104       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.199       |\n",
      "|    explained_variance   | 0.972        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0296       |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | 0.000235     |\n",
      "|    value_loss           | 0.0709       |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.32\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.02\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -5.94        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 510          |\n",
      "|    iterations           | 69           |\n",
      "|    time_elapsed         | 277          |\n",
      "|    total_timesteps      | 141312       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012496378 |\n",
      "|    clip_fraction        | 0.0243       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.28        |\n",
      "|    explained_variance   | 0.975        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0159       |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.000361    |\n",
      "|    value_loss           | 0.0777       |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.89\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.21\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -5.99        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 510          |\n",
      "|    iterations           | 70           |\n",
      "|    time_elapsed         | 280          |\n",
      "|    total_timesteps      | 143360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012400314 |\n",
      "|    clip_fraction        | 0.0113       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.258       |\n",
      "|    explained_variance   | 0.966        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0165       |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -0.00157     |\n",
      "|    value_loss           | 0.073        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.82\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.65\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -5.87        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 510          |\n",
      "|    iterations           | 71           |\n",
      "|    time_elapsed         | 284          |\n",
      "|    total_timesteps      | 145408       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019677698 |\n",
      "|    clip_fraction        | 0.0294       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.212       |\n",
      "|    explained_variance   | 0.933        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0602       |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    value_loss           | 0.131        |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.81\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.65\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -5.79        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 510          |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 288          |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010268837 |\n",
      "|    clip_fraction        | 0.0137       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.184       |\n",
      "|    explained_variance   | 0.971        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0357       |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.000271    |\n",
      "|    value_loss           | 0.0794       |\n",
      "------------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.75\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.74\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 19         |\n",
      "|    ep_rew_mean          | -5.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 510        |\n",
      "|    iterations           | 73         |\n",
      "|    time_elapsed         | 292        |\n",
      "|    total_timesteps      | 149504     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00048744 |\n",
      "|    clip_fraction        | 0.00791    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.167     |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0646     |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | 0.000318   |\n",
      "|    value_loss           | 0.0781     |\n",
      "----------------------------------------\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -5.34\n",
      "Best mean reward: -5.01 - Last mean reward per episode: -6.57\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -5.84        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 510          |\n",
      "|    iterations           | 74           |\n",
      "|    time_elapsed         | 296          |\n",
      "|    total_timesteps      | 151552       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007030083 |\n",
      "|    clip_fraction        | 0.0114       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.186       |\n",
      "|    explained_variance   | 0.964        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.041        |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -0.000423    |\n",
      "|    value_loss           | 0.0946       |\n",
      "------------------------------------------\n",
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Best mean reward: -inf - Last mean reward per episode: -8.95\n",
      "Saving new best model to active_teaching/active_teaching_easy.zip\n",
      "Best mean reward: -8.95 - Last mean reward per episode: -8.94\n",
      "Saving new best model to active_teaching/active_teaching_easy.zip\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19       |\n",
      "|    ep_rew_mean     | -9.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 559      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Best mean reward: -8.94 - Last mean reward per episode: -6.04\n",
      "Saving new best model to active_teaching/active_teaching_easy.zip\n",
      "Best mean reward: -6.04 - Last mean reward per episode: -6.32\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -9.09       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 539         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013052655 |\n",
      "|    clip_fraction        | 0.0936      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.00403    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.13        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0043     |\n",
      "|    value_loss           | 3.45        |\n",
      "-----------------------------------------\n",
      "Best mean reward: -6.04 - Last mean reward per episode: -6.01\n",
      "Saving new best model to active_teaching/active_teaching_easy.zip\n",
      "Best mean reward: -6.01 - Last mean reward per episode: -5.64\n",
      "Saving new best model to active_teaching/active_teaching_easy.zip\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 19         |\n",
      "|    ep_rew_mean          | -9.07      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 532        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01425973 |\n",
      "|    clip_fraction        | 0.0876     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.59      |\n",
      "|    explained_variance   | 0.803      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.248      |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.00668   |\n",
      "|    value_loss           | 0.519      |\n",
      "----------------------------------------\n",
      "Best mean reward: -5.64 - Last mean reward per episode: -6.70\n",
      "Best mean reward: -5.64 - Last mean reward per episode: -6.14\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -8.84       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 529         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011799768 |\n",
      "|    clip_fraction        | 0.0933      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.178       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.009      |\n",
      "|    value_loss           | 0.423       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.64 - Last mean reward per episode: -5.76\n",
      "Best mean reward: -5.64 - Last mean reward per episode: -5.78\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -8.91       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 526         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008149696 |\n",
      "|    clip_fraction        | 0.0383      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.15        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00384    |\n",
      "|    value_loss           | 0.422       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -5.64 - Last mean reward per episode: -4.55\n",
      "Saving new best model to active_teaching/active_teaching_easy.zip\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.39\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -8.92       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 525         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013252457 |\n",
      "|    clip_fraction        | 0.0895      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.2         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00775    |\n",
      "|    value_loss           | 0.439       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.93\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.08\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -8.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 525         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011847474 |\n",
      "|    clip_fraction        | 0.0636      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.127       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00754    |\n",
      "|    value_loss           | 0.342       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.81\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.08\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -8.75       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 524         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010906127 |\n",
      "|    clip_fraction        | 0.0404      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.919       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.162       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00435    |\n",
      "|    value_loss           | 0.438       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.68\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.03\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -8.54       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 524         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011945499 |\n",
      "|    clip_fraction        | 0.0964      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.257       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00817    |\n",
      "|    value_loss           | 0.488       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.73\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.94\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -8.48        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 523          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074805943 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.897        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.271        |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00667     |\n",
      "|    value_loss           | 0.533        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.06\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.15\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -8.36       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 522         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009145603 |\n",
      "|    clip_fraction        | 0.0966      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.206       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00593    |\n",
      "|    value_loss           | 0.451       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.17\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.88\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -7.67       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 522         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011261957 |\n",
      "|    clip_fraction        | 0.0481      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.35        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00329    |\n",
      "|    value_loss           | 0.626       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.76\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.47\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -7.74       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 522         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013224812 |\n",
      "|    clip_fraction        | 0.0992      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.306       |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00699    |\n",
      "|    value_loss           | 0.723       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.06\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.29\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -7.31       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 522         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004933196 |\n",
      "|    clip_fraction        | 0.0271      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.826       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.391       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0017     |\n",
      "|    value_loss           | 0.745       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.03\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.72\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -7.24       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 522         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011097952 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.947      |\n",
      "|    explained_variance   | 0.74        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.57        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0092     |\n",
      "|    value_loss           | 1.01        |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.38\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.37\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -7.22        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 522          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 62           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070453566 |\n",
      "|    clip_fraction        | 0.0398       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.983       |\n",
      "|    explained_variance   | 0.752        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.496        |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00471     |\n",
      "|    value_loss           | 0.939        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.00\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.19\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.92        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 522          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 66           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027696404 |\n",
      "|    clip_fraction        | 0.0125       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.899       |\n",
      "|    explained_variance   | 0.792        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.342        |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.000793    |\n",
      "|    value_loss           | 0.741        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.64\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.74\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.88        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 522          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 70           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060572335 |\n",
      "|    clip_fraction        | 0.0639       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.928       |\n",
      "|    explained_variance   | 0.749        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.531        |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00627     |\n",
      "|    value_loss           | 0.825        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.03\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.93\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.87        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 521          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 74           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024818927 |\n",
      "|    clip_fraction        | 0.0254       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.829       |\n",
      "|    explained_variance   | 0.78         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.303        |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00244     |\n",
      "|    value_loss           | 0.705        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.08\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.23\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 19         |\n",
      "|    ep_rew_mean          | -6.26      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 521        |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 78         |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00233559 |\n",
      "|    clip_fraction        | 0.0299     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.799     |\n",
      "|    explained_variance   | 0.746      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.474      |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.00219   |\n",
      "|    value_loss           | 0.842      |\n",
      "----------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.70\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.43\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.52\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.69        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 520          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 82           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022373009 |\n",
      "|    clip_fraction        | 0.0279       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.717       |\n",
      "|    explained_variance   | 0.796        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.307        |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00127     |\n",
      "|    value_loss           | 0.591        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.35\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.42\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.52        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 520          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 86           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019729622 |\n",
      "|    clip_fraction        | 0.0147       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.801       |\n",
      "|    explained_variance   | 0.781        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.453        |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00028     |\n",
      "|    value_loss           | 0.683        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.80\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.88\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.79        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 520          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 90           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043264115 |\n",
      "|    clip_fraction        | 0.0392       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.798       |\n",
      "|    explained_variance   | 0.736        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.328        |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.0038      |\n",
      "|    value_loss           | 0.789        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.72\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.65\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.35        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 520          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 94           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033374918 |\n",
      "|    clip_fraction        | 0.0324       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.752       |\n",
      "|    explained_variance   | 0.773        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.347        |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00195     |\n",
      "|    value_loss           | 0.733        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.20\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.10\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.43        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 520          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 98           |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023540324 |\n",
      "|    clip_fraction        | 0.0287       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.659       |\n",
      "|    explained_variance   | 0.779        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.28         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00161     |\n",
      "|    value_loss           | 0.664        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.41\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.05\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.29       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 520         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 102         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004003697 |\n",
      "|    clip_fraction        | 0.0232      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.615      |\n",
      "|    explained_variance   | 0.777       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.298       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.000995   |\n",
      "|    value_loss           | 0.639       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.77\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.52\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.29        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 520          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 106          |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014215434 |\n",
      "|    clip_fraction        | 0.0124       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.603       |\n",
      "|    explained_variance   | 0.769        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.378        |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | 2.3e-05      |\n",
      "|    value_loss           | 0.684        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.94\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.43\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.53        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 520          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 110          |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020980774 |\n",
      "|    clip_fraction        | 0.0237       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.663       |\n",
      "|    explained_variance   | 0.799        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.348        |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    value_loss           | 0.581        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.69\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.40\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.89        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 520          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 114          |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019813527 |\n",
      "|    clip_fraction        | 0.0257       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.721       |\n",
      "|    explained_variance   | 0.779        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.301        |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00154     |\n",
      "|    value_loss           | 0.628        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.76\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.67\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.36       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 520         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 118         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005284371 |\n",
      "|    clip_fraction        | 0.0489      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.775      |\n",
      "|    explained_variance   | 0.825       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.303       |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00355    |\n",
      "|    value_loss           | 0.554       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.83\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.38\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.33       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 520         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 122         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003706546 |\n",
      "|    clip_fraction        | 0.037       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.761      |\n",
      "|    explained_variance   | 0.779       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.27        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0024     |\n",
      "|    value_loss           | 0.673       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.56\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -4.98\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.57        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 520          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 125          |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042792317 |\n",
      "|    clip_fraction        | 0.03         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.754       |\n",
      "|    explained_variance   | 0.711        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.453        |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.0013      |\n",
      "|    value_loss           | 0.794        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.24\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.75\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.53       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 520         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 129         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005569176 |\n",
      "|    clip_fraction        | 0.0488      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.737      |\n",
      "|    explained_variance   | 0.777       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.357       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00366    |\n",
      "|    value_loss           | 0.678       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.40\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -4.78\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.28        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 520          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 133          |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036020484 |\n",
      "|    clip_fraction        | 0.0309       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.755       |\n",
      "|    explained_variance   | 0.773        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.375        |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00257     |\n",
      "|    value_loss           | 0.677        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.48\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -4.69\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 520          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 137          |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043080263 |\n",
      "|    clip_fraction        | 0.0277       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.731       |\n",
      "|    explained_variance   | 0.768        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.367        |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00187     |\n",
      "|    value_loss           | 0.654        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.66\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.55\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.21        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 520          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 141          |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031144517 |\n",
      "|    clip_fraction        | 0.0259       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.738       |\n",
      "|    explained_variance   | 0.792        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.239        |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.0015      |\n",
      "|    value_loss           | 0.596        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.65\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.69\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.48        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 519          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 145          |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022170637 |\n",
      "|    clip_fraction        | 0.0384       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.744       |\n",
      "|    explained_variance   | 0.824        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.311        |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00196     |\n",
      "|    value_loss           | 0.486        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.67\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.88\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.54        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 520          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 149          |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032911384 |\n",
      "|    clip_fraction        | 0.0338       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.761       |\n",
      "|    explained_variance   | 0.807        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.311        |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00251     |\n",
      "|    value_loss           | 0.523        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.26\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.12\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 19         |\n",
      "|    ep_rew_mean          | -6.37      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 519        |\n",
      "|    iterations           | 39         |\n",
      "|    time_elapsed         | 153        |\n",
      "|    total_timesteps      | 79872      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00537815 |\n",
      "|    clip_fraction        | 0.0315     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.758     |\n",
      "|    explained_variance   | 0.839      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.245      |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.00149   |\n",
      "|    value_loss           | 0.493      |\n",
      "----------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.68\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.11\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.65        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 520          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 157          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041727633 |\n",
      "|    clip_fraction        | 0.0175       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.783       |\n",
      "|    explained_variance   | 0.813        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.207        |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -6.19e-05    |\n",
      "|    value_loss           | 0.54         |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.16\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.34\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.31        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 519          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 161          |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037938429 |\n",
      "|    clip_fraction        | 0.0238       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.78        |\n",
      "|    explained_variance   | 0.761        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.344        |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.000748    |\n",
      "|    value_loss           | 0.735        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.41\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.70\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.08\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.75        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 519          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 165          |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049825856 |\n",
      "|    clip_fraction        | 0.0366       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.758       |\n",
      "|    explained_variance   | 0.807        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.351        |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00186     |\n",
      "|    value_loss           | 0.541        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.26\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.63\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 19        |\n",
      "|    ep_rew_mean          | -6.47     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 519       |\n",
      "|    iterations           | 43        |\n",
      "|    time_elapsed         | 169       |\n",
      "|    total_timesteps      | 88064     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0048991 |\n",
      "|    clip_fraction        | 0.0446    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.81     |\n",
      "|    explained_variance   | 0.797     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.246     |\n",
      "|    n_updates            | 420       |\n",
      "|    policy_gradient_loss | -0.00307  |\n",
      "|    value_loss           | 0.642     |\n",
      "---------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.87\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -7.20\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.43       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 519         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 173         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006133202 |\n",
      "|    clip_fraction        | 0.0258      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.759      |\n",
      "|    explained_variance   | 0.802       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.194       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00181    |\n",
      "|    value_loss           | 0.566       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.25\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.97\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 19        |\n",
      "|    ep_rew_mean          | -6.21     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 519       |\n",
      "|    iterations           | 45        |\n",
      "|    time_elapsed         | 177       |\n",
      "|    total_timesteps      | 92160     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0040073 |\n",
      "|    clip_fraction        | 0.0454    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.716    |\n",
      "|    explained_variance   | 0.818     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.281     |\n",
      "|    n_updates            | 440       |\n",
      "|    policy_gradient_loss | -0.00254  |\n",
      "|    value_loss           | 0.534     |\n",
      "---------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -4.83\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.44\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 519          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 181          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053298324 |\n",
      "|    clip_fraction        | 0.038        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.633       |\n",
      "|    explained_variance   | 0.814        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.173        |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00267     |\n",
      "|    value_loss           | 0.532        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.50\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.08\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 19         |\n",
      "|    ep_rew_mean          | -6.26      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 519        |\n",
      "|    iterations           | 47         |\n",
      "|    time_elapsed         | 185        |\n",
      "|    total_timesteps      | 96256      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00544341 |\n",
      "|    clip_fraction        | 0.0895     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.604     |\n",
      "|    explained_variance   | 0.851      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.196      |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.00997   |\n",
      "|    value_loss           | 0.418      |\n",
      "----------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.64\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.44\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.42        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 519          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 189          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019097724 |\n",
      "|    clip_fraction        | 0.0211       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.578       |\n",
      "|    explained_variance   | 0.849        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.166        |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.000552    |\n",
      "|    value_loss           | 0.419        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.76\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.52\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 519          |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 193          |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030684453 |\n",
      "|    clip_fraction        | 0.0245       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.577       |\n",
      "|    explained_variance   | 0.842        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.23         |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.000896    |\n",
      "|    value_loss           | 0.439        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.75\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.74\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.24       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 519         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 197         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002608845 |\n",
      "|    clip_fraction        | 0.0109      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.598      |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.166       |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -8.38e-05   |\n",
      "|    value_loss           | 0.401       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.17\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.94\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.19        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 519          |\n",
      "|    iterations           | 51           |\n",
      "|    time_elapsed         | 201          |\n",
      "|    total_timesteps      | 104448       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035192822 |\n",
      "|    clip_fraction        | 0.0436       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.565       |\n",
      "|    explained_variance   | 0.852        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.171        |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.00361     |\n",
      "|    value_loss           | 0.43         |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.91\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -7.03\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.42        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 519          |\n",
      "|    iterations           | 52           |\n",
      "|    time_elapsed         | 205          |\n",
      "|    total_timesteps      | 106496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020155343 |\n",
      "|    clip_fraction        | 0.0198       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.585       |\n",
      "|    explained_variance   | 0.873        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.165        |\n",
      "|    n_updates            | 510          |\n",
      "|    policy_gradient_loss | -0.000825    |\n",
      "|    value_loss           | 0.351        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.59\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.83\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.32        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 519          |\n",
      "|    iterations           | 53           |\n",
      "|    time_elapsed         | 208          |\n",
      "|    total_timesteps      | 108544       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029483517 |\n",
      "|    clip_fraction        | 0.0407       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.621       |\n",
      "|    explained_variance   | 0.874        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.118        |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -0.00309     |\n",
      "|    value_loss           | 0.36         |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.13\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.17\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.48       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 519         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 212         |\n",
      "|    total_timesteps      | 110592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003539858 |\n",
      "|    clip_fraction        | 0.0321      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.643      |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.223       |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.00168    |\n",
      "|    value_loss           | 0.356       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.14\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.09\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.64        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 519          |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 216          |\n",
      "|    total_timesteps      | 112640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025808322 |\n",
      "|    clip_fraction        | 0.0177       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.646       |\n",
      "|    explained_variance   | 0.826        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.258        |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.000612    |\n",
      "|    value_loss           | 0.479        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.09\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.79\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.15        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 519          |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 220          |\n",
      "|    total_timesteps      | 114688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038447129 |\n",
      "|    clip_fraction        | 0.0233       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.624       |\n",
      "|    explained_variance   | 0.806        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.287        |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.000698    |\n",
      "|    value_loss           | 0.577        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.31\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.24\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.09        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 519          |\n",
      "|    iterations           | 57           |\n",
      "|    time_elapsed         | 224          |\n",
      "|    total_timesteps      | 116736       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035909612 |\n",
      "|    clip_fraction        | 0.0308       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.582       |\n",
      "|    explained_variance   | 0.836        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.29         |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.00229     |\n",
      "|    value_loss           | 0.442        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.47\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.08\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.49       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 519         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 228         |\n",
      "|    total_timesteps      | 118784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003175112 |\n",
      "|    clip_fraction        | 0.0272      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.553      |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.159       |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.00272    |\n",
      "|    value_loss           | 0.41        |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.21\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.04\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.27        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 519          |\n",
      "|    iterations           | 59           |\n",
      "|    time_elapsed         | 232          |\n",
      "|    total_timesteps      | 120832       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047304225 |\n",
      "|    clip_fraction        | 0.0318       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.594       |\n",
      "|    explained_variance   | 0.806        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.151        |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.00114     |\n",
      "|    value_loss           | 0.553        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.87\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.23\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.26        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 519          |\n",
      "|    iterations           | 60           |\n",
      "|    time_elapsed         | 236          |\n",
      "|    total_timesteps      | 122880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033998254 |\n",
      "|    clip_fraction        | 0.0177       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.601       |\n",
      "|    explained_variance   | 0.825        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.249        |\n",
      "|    n_updates            | 590          |\n",
      "|    policy_gradient_loss | -0.000786    |\n",
      "|    value_loss           | 0.509        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.95\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.99\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.21        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 519          |\n",
      "|    iterations           | 61           |\n",
      "|    time_elapsed         | 240          |\n",
      "|    total_timesteps      | 124928       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014419407 |\n",
      "|    clip_fraction        | 0.0225       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.604       |\n",
      "|    explained_variance   | 0.857        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.254        |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.000795    |\n",
      "|    value_loss           | 0.404        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.60\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.09\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.27        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 519          |\n",
      "|    iterations           | 62           |\n",
      "|    time_elapsed         | 244          |\n",
      "|    total_timesteps      | 126976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012813071 |\n",
      "|    clip_fraction        | 0.0197       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.595       |\n",
      "|    explained_variance   | 0.825        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.211        |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | 0.000331     |\n",
      "|    value_loss           | 0.471        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.71\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.62\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.39\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.28        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 518          |\n",
      "|    iterations           | 63           |\n",
      "|    time_elapsed         | 248          |\n",
      "|    total_timesteps      | 129024       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046766493 |\n",
      "|    clip_fraction        | 0.0459       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.604       |\n",
      "|    explained_variance   | 0.821        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.302        |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.00291     |\n",
      "|    value_loss           | 0.462        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.09\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.45\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.61        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 518          |\n",
      "|    iterations           | 64           |\n",
      "|    time_elapsed         | 252          |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043668267 |\n",
      "|    clip_fraction        | 0.0489       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.611       |\n",
      "|    explained_variance   | 0.838        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0935       |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.00299     |\n",
      "|    value_loss           | 0.443        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.68\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.40\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.45       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 518         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 256         |\n",
      "|    total_timesteps      | 133120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004567812 |\n",
      "|    clip_fraction        | 0.038       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.613      |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.224       |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.00221    |\n",
      "|    value_loss           | 0.429       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.34\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.88\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.39        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 518          |\n",
      "|    iterations           | 66           |\n",
      "|    time_elapsed         | 260          |\n",
      "|    total_timesteps      | 135168       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046048313 |\n",
      "|    clip_fraction        | 0.0246       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.611       |\n",
      "|    explained_variance   | 0.83         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.181        |\n",
      "|    n_updates            | 650          |\n",
      "|    policy_gradient_loss | -0.0014      |\n",
      "|    value_loss           | 0.483        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.89\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -7.13\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.29        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 518          |\n",
      "|    iterations           | 67           |\n",
      "|    time_elapsed         | 264          |\n",
      "|    total_timesteps      | 137216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043829246 |\n",
      "|    clip_fraction        | 0.0408       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.619       |\n",
      "|    explained_variance   | 0.836        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.258        |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.00259     |\n",
      "|    value_loss           | 0.481        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.45\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.30\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.73        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 518          |\n",
      "|    iterations           | 68           |\n",
      "|    time_elapsed         | 268          |\n",
      "|    total_timesteps      | 139264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035285922 |\n",
      "|    clip_fraction        | 0.0202       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.604       |\n",
      "|    explained_variance   | 0.835        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.266        |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | -0.000252    |\n",
      "|    value_loss           | 0.467        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.64\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.20\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.36        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 517          |\n",
      "|    iterations           | 69           |\n",
      "|    time_elapsed         | 273          |\n",
      "|    total_timesteps      | 141312       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059369234 |\n",
      "|    clip_fraction        | 0.0522       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.614       |\n",
      "|    explained_variance   | 0.821        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.325        |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.00322     |\n",
      "|    value_loss           | 0.52         |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.66\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.21\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.57        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 517          |\n",
      "|    iterations           | 70           |\n",
      "|    time_elapsed         | 277          |\n",
      "|    total_timesteps      | 143360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023005519 |\n",
      "|    clip_fraction        | 0.0245       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.605       |\n",
      "|    explained_variance   | 0.849        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.256        |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -0.00136     |\n",
      "|    value_loss           | 0.425        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.24\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.36\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 19          |\n",
      "|    ep_rew_mean          | -6.36       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 517         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 281         |\n",
      "|    total_timesteps      | 145408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007580665 |\n",
      "|    clip_fraction        | 0.0346      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.618      |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.213       |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.00225    |\n",
      "|    value_loss           | 0.433       |\n",
      "-----------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.84\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.15\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.38        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 516          |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 285          |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064497646 |\n",
      "|    clip_fraction        | 0.0301       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.576       |\n",
      "|    explained_variance   | 0.799        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.288        |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.00122     |\n",
      "|    value_loss           | 0.551        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -6.23\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.79\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.27        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 516          |\n",
      "|    iterations           | 73           |\n",
      "|    time_elapsed         | 289          |\n",
      "|    total_timesteps      | 149504       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050607873 |\n",
      "|    clip_fraction        | 0.0275       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.585       |\n",
      "|    explained_variance   | 0.818        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.165        |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.00151     |\n",
      "|    value_loss           | 0.494        |\n",
      "------------------------------------------\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -5.97\n",
      "Best mean reward: -4.55 - Last mean reward per episode: -7.10\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 19           |\n",
      "|    ep_rew_mean          | -6.37        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 516          |\n",
      "|    iterations           | 74           |\n",
      "|    time_elapsed         | 293          |\n",
      "|    total_timesteps      | 151552       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045087123 |\n",
      "|    clip_fraction        | 0.0432       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.566       |\n",
      "|    explained_variance   | 0.838        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.235        |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -0.00233     |\n",
      "|    value_loss           | 0.444        |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "baseline_policy = train_influence_policy('baseline')\n",
    "oracle_policy = train_influence_policy('oracle')\n",
    "estimated_policy = train_influence_policy('estimated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_map = {'baseline': baseline_policy, 'oracle': oracle_policy, 'estimated': estimated_policy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline passive cumulative rewards: -9.411657142380797\n",
      "baseline active cumulative rewards: -4.433160846722234\n",
      "oracle passive cumulative rewards: -9.388327432708623\n",
      "oracle active cumulative rewards: -6.137689690725613\n",
      "estimated passive cumulative rewards: -9.266128905213758\n",
      "estimated active cumulative rewards: -4.494985470651275\n"
     ]
    }
   ],
   "source": [
    "for influence_type in ['baseline', 'oracle', 'estimated']:\n",
    "    env_active = rollout_env(policy_map[influence_type], 'active_teaching', influence_type)\n",
    "    env_passive = rollout_env(policy_map[influence_type], 'passive_teaching', influence_type)\n",
    "\n",
    "    print(f'{influence_type} passive cumulative rewards:', np.sum(env_passive.current_demo_reward_traj))\n",
    "    print(f'{influence_type} active cumulative rewards:', np.sum(env_active.current_demo_reward_traj))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robot reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(env_active.current_demo_reward_traj,'g',markersize = 3, label='Active learning')\n",
    "plt.plot(env_passive.current_demo_reward_traj,'b',markersize = 3, label='Passive teaching')\n",
    "plt.xlim([0, env_passive.episode_length])\n",
    "#plt.ylim([-0.3,0.05])\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('Robot reward')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\theta_H$ error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_human_mental_error_hist = 1.0 - np.array(env_passive.current_demo_human_mental_state_traj)[:,0,0]\n",
    "dynamic_human_mental_error_hist = 1.0 - np.array(env_active.current_demo_human_mental_state_traj)[:,0,0]\n",
    "plt.plot( np.abs(static_human_mental_error_hist),'b-',markersize = 3, label='Passive learning')\n",
    "plt.plot( np.abs(dynamic_human_mental_error_hist),'g-',markersize = 3, label='Active teaching')\n",
    "plt.xlim([0, env_passive.episode_length])\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('Human mental model error')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robot task reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(env_passive.current_demo_task_reward_traj,'b-',markersize = 3, label='Passive teaching')\n",
    "plt.plot(env_active.current_demo_task_reward_traj,'g-',markersize = 3, label='Active teaching')\n",
    "plt.xlim([0, env_passive.episode_length])\n",
    "plt.ylim([-0.5,0])\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('Robot task cost')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robot action reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(env_passive.current_demo_action_reward_traj,'b-',markersize = 3, label='Passive teaching')\n",
    "plt.plot(env_active.current_demo_action_reward_traj,'g-',markersize = 3, label='Active teaching')\n",
    "plt.xlim([0, env_passive.episode_length])\n",
    "plt.ylim([0 , 0.5])\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('Robot action cost')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human optimal action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(env_passive.current_demo_human_action_opt_traj,'bo',markersize = 3, label='Passive learning')\n",
    "plt.plot(env_active.current_demo_human_action_opt_traj,'go',markersize = 3, label='Active teaching')\n",
    "plt.xlim([0, env_passive.episode_length])\n",
    "plt.ylim([0,0.1])\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('human action opt')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
